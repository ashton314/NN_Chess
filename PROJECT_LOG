Start: Wed Sep 11 08:00:02 MDT 2013
Stop:  Wed Sep 11 08:20:49 MDT 2013
Total: ~0:20

This is to help me keep track of the project, and so that I don't have
to clutter up the git commit with these longer entries.

Today I'm working on implementing the backprop algorithm. I'm using
that PDF series I found. Once I get that working I'll write some tests
using the examples in the PDF.
.

Start: Thu Sep 12 17:29:03 MDT 2013
Stop:  Thu Sep 12 20:32:24 MDT 2013
Subtractions: 1:00
Total: ~2:00

Finished writing a crude back propogation algorithm! The interface is
very messy, but it passed the tests. Now that I've got the very basics
of the network working, I'll try my network model out on some more
complicated examples, and test doing "real" learning with it. For fun,
I could teach it to play tic-tac-toe.

I'm wanting to rewrite the network simulator. The model I've got right
now is kinda messy and probably fairly inefficent. I think I'll try
writing another closure-based network system, but this time do it
better.

Eventually, I'd like to write the network with closures that compile
into some nice, fast code. I might wait to do this until after I've
got other elements of this project working, such as the chess engine
and the Negascout framework.
.

Start: Sat Sep 14 17:42:30 MDT 2013
Stop:  Sat Sep 14 20:34:51 MDT 2013
Total: ~3:00

I didn't like how ugly the closure nodes looked, so I started working
on a version that uses scheme macros. To do this, I picked a syntax
for the networks that I liked better than the old syntax, and then
basically wrote my own Domain Specific Language to implement the new
syntax.

I haven't ever done much work with Scheme's pattern matching
macros. I've used Common Lisp's macros extensivly, and thanks to Paul
Graham's book "On Lisp", I got to the point where I wrote an extremely
complex macro system that allowed me to write a very concise natural
language parser. (By concise I mean it had minimal boiler plate and
was easy to read.) The macros I'm writing for this are the largest
I've written to date (in Scheme of course), but they are far from the
most complex out there.
.

Start: Sat Sep 14 21:17:45 MDT 2013
Stop:  Sat Sep 14 23:25:43 MDT 2013
Subtractions: 0:10
Total: ~2:00

I got back propagation working with my macro tests!! For one day, I've
done a lot. I'm going to go to bed now...
.

Start: Mon Sep 16 20:55:00 MDT 2013
Stop:  Mon Sep 16 22:00:47 MDT 2013
Total: 1:05

Working on doing character recognition. Not going so well... I don't
know what's wrong. I need to debug this before I move onto my chess
engines etc.
.

Start: Tue Sep 17 19:41:31 MDT 2013
Stop:  Tue Sep 17 20:57:54 MDT 2013
Subractions: 0:00
Total: ~1:00

Debugging the character recognition net. I wrote some simpler pattern
learning networks, which worked. I'm going to get a server and train
my character recognizer heavily.
.

Start: Wed Sep 18 08:08:18 MDT 2013
Stop:  Wed Sep 18 08:21:49 MDT 2013
Total: 0:13

Worked on compiling the net into closures.
.

Start: Thu Sep 19 19:16:39 MDT 2013
Stop:  Thu Sep 19 19:21:22 MDT 2013
Start: Thu Sep 19 20:32:48 MDT 2013
Stop:  Thu Sep 19 22:19:47 MDT 2013
Total: ~1:45

Worked on debugging the new system. I've fixed quite a few minor bugs,
but I have no idea as to how many I've got left. Right now the values
for nodes (at least the output nodes) are not being set
correctly. Obviously, this is a problem as I query those nodes to get
the output of the neural net. I'm suspecting the problem is where I
have the logic for getting node values. (See the closure construction
of the nodes.)
.

Start: Fri Sep 20 20:03:01 MDT 2013
Stop:  Fri Sep 20 21:45:30 MDT 2013

Major debug session. Still not working. I'm absolutly mystified with a
particular problem: either I'm missing a very subtle bug with my usage
of `case', or the `case' macro (or even worse, something even more
low-level) has a bug in it. The bug appeared when I added a `let'
arround the place where I'm using `case'... I'm considering replacing
my usage of `case' with a `cond' expression so I can poke around in
the internals a bit more.
.

Start: Sat Sep 21 15:55:32 MDT 2013
Stop:  Sat Sep 21 17:38:02 MDT 2013

Got compiling into closures working. I need to do a speed trial, and
try out my character recognition with the new system.

That was one heck of a debug session!
.

Start: Sat Sep 21 20:51:32 MDT 2013
Stop:  Sat Sep 21 21:40:02 MDT 2013

Going to do some profiling.

...

I am absolutely mystified as to why on earth the new compiled macros
are running slower than the old ones. ?!! I need a profiler. Bad.
.

Start: Tue Sep 24 17:57:57 MDT 2013
Stop:  Tue Sep 24 19:10:03 MDT 2013

I'm going to try writing my feed-forward networks with matricies. This
should be pretty dang fast if I can code it right.
.

Start: Wed Sep 25 07:40:15 MDT 2013
Stop:  Wed Sep 25 08:20:56 MDT 2013

More work on ff-networks with matricies. There are no native matricies
for MIT Scheme, so I have to implement them myself with the native
vectors.
.

Start: Thu Sep 26 18:33:58 MDT 2013
Stop:  Thu Sep 26 20:02:00 MDT 2013

Working on implementing the backprop algorithm with matricies.
.

Start: Mon Sep 30 16:12:20 MDT 2013
Stop:  Mon Sep 30 19:17:13 MDT 2013

Ditto.

I'm still getting some errors. My calculation of the value
"forward-errors" on line 152 does not match the correct
algorithm. Compare with (δαWAα + δβWAβ) and friends on page 19 of
chapter 3 of the neural net tutorial PDFs.

I'm too tired right now to figure out the correct matrix
computations.
.

Start: Tue Oct  1 20:28:32 MDT 2013
Stop:  Tue Oct  1 21:08:36 MDT 2013
Start: Tue Oct  1 21:19:18 MDT 2013
Stop:  Tue Oct  1 21:24:52 MDT 2013

Still working on backprop algorithm.

IT'S WORKING!!! I think...

Nope. Something is still wrong.
.

Start: Wed Oct  2 07:42:59 MDT 2013
Stop:  Wed Oct  2 08:22:21 MDT 2013
Start: Wed Oct  2 09:26:32 MDT 2013
Stop:  Wed Oct  2 10:07:28 MDT 2013

It /looks/ like the mathematics are correct, but for some reason the
numbers are off after about 3 decimal places. Still not learning.

I think the decimal place bug is due to I'm working at a much higher
precision.
.

Start: Wed Oct  2 13:15:21 MDT 2013
Stop:  Wed Oct  2 13:21:34 MDT 2013
Start: Wed Oct  2 17:49:16 MDT 2013
Stop:  Wed Oct  2 18:52:55 MDT 2013

Rewriting the back prop algorithm. The code is a /tremendous/ amount
cleaner than the last one. See the "BUG:" note for my current
problem.
.

Start: Wed Oct 16 18:38:31 MDT 2013
Stop:  Wed Oct 16 18:50:31 MDT 2013
Start: Wed Oct 16 20:45:54 MDT 2013
Stop:  Wed Oct 16 23:21:17 MDT 2013

Recommencing work on progress after a lengthy break.

I've got learning with matricies working... sort of. The network is
only learning the first pattern given to it, which makes me think that
the error is caused by me not updating the values or the targets of
nodes somewhere...
I did some more checking, and I got different results from what I was
expecting: I trained the net once right at the start with pattern 1.
Then I trained it 1000 times with pattern 0. I expected that the
net would kinda learn pattern 1, and not learn pattern 0. However, the
network learned pattern 0, but returned roughly the same result for
pattern 1. I still think it's side effects.

I HAVE DONE SOMETHING VERY WRONG WITH MY DATA STRUCTURES!! Two x-rays:

Before /any/ activity:
1 ]=> (test-net01 'x-ray)
-------------- FULL DUMP FOR: 'test-net01' --------------
LAYER DUMP:
#(.1 .1 .1 .1)#(.1 .1 .1 .1)#(.1 .1 .1 .1)#(.1 .1 .1 .1)#(.1 .1 .1 .1)
#(#(.1 .1 .1 .1 .1) #(.1 .1 .1 .1 .1) #(.1 .1 .1 .1 .1) #(.1 .1 .1 .1
.1))
ERROR DUMP:
#(0 0 0 0)#(0 0 0 0)#(0 0 0 0)#(0 0 0 0)#(0 0 0 0)
#(0 0 0 0)
VALUE DUMP:
#(0 0 0 0)#(0 0 0 0)#(0 0 0 0)#(0 0 0 0)#(0 0 0 0)
#(#(0 0 0 0 0) #(0 0 0 0 0) #(0 0 0 0 0) #(0 0 0 0 0))
Reverse lists are up-to-date.
;Unspecified return value

1 ]=> (train-01-0 1)
;Value: #t

1 ]=> (test-net01 'x-ray)
-------------- FULL DUMP FOR: 'test-net01' --------------
LAYER DUMP:
#(.10668521032893902 .1 .1 .10668521032893902)#(.10668521032893902 .1
.1 .10668521032893902)#(.10668521032893902 .1 .1
.10668521032893902)#(.10668521032893902 .1 .1
.10668521032893902)#(.10668521032893902 .1 .1 .10668521032893902)
#(#(.15823362381053097 .15823362381053097 .15823362381053097
.15823362381053097 .15823362381053097)
  #(2.3340011090897797e-2 2.3340011090897797e-2 2.3340011090897797e-2
  2.3340011090897797e-2 2.3340011090897797e-2)
  #(2.3340011090897797e-2 2.3340011090897797e-2 2.3340011090897797e-2
  2.3340011090897797e-2 2.3340011090897797e-2)
  #(.15823362381053097 .15823362381053097 .15823362381053097
  .15823362381053097 .15823362381053097))
ERROR DUMP:
6.685210328939012e-36.685210328939012e-36.685210328939012e-36.685210328939012e-36.685210328939012e-3
#(.10591128248738689 -.13942387935960118 -.13942387935960118
.10591128248738689)
VALUE DUMP:
.549833997312478.549833997312478.549833997312478.549833997312478.549833997312478
#(.5682996204455919 .5682996204455919 .5682996204455919
.5682996204455919)
Reverse lists are up-to-date.
;Unspecified return value

Notice how before any activity, the values of _a_ layer are held in a
list of arrays. But after training, the values of _a_ layer are held
in a flat list! (Also note how they are all the same: this, I think is
very wrong. (Maybe not for the first training session... or maybe
yes. I don't know.)) Values should be a flat list, and errors need to
be a list of arrays.

OK, this is interesting. Every other training session, the layer
values are different from one another! Hhmmm...

1 ]=> (train-01 10)
Layer: #(.549833997312478 .549833997312478 .549833997312478 .549833997312478 .549833997312478)
Layer: #(.5682996204455919 .5682996204455919 .5682996204455919 .5682996204455919)
Layer: #(.549833997312478 .549833997312478 .549833997312478 .549833997312478 .549833997312478)
Layer: #(.6070696428491587 .5160359129077615 .5160359129077615 .6070696428491587)
Layer: #(.5531411451842421 .5531411451842421 .5531411451842421 .5531411451842421 .5531411451842421)
Layer: #(.5541421639397078 .5617705501752163 .5617705501752163 .5541421639397078)
Layer: #(.5497040086343132 .5497040086343132 .5497040086343132 .5497040086343132 .5497040086343132)
Layer: #(.5947277487620963 .5091361718820695 .5091361718820695 .5947277487620963)
Layer: #(.5564459877924409 .5564459877924409 .5564459877924409 .5564459877924409 .5564459877924409)
Layer: #(.5421537279531315 .5559192168507334 .5559192168507334 .5421537279531315)
Layer: #(.5499970779641044 .5499970779641044 .5499970779641044 .5499970779641044 .5499970779641044)
Layer: #(.5844245663272676 .5030009403207444 .5030009403207444 .5844245663272676)
Layer: #(.5597310060673503 .5597310060673503 .5597310060673503 .5597310060673503 .5597310060673503)
Layer: #(.532084739142932 .5506905464203923 .5506905464203923 .532084739142932)
Layer: #(.5506131434950248 .5506131434950248 .5506131434950248 .5506131434950248 .5506131434950248)
Layer: #(.5758942885575231 .49754429270011086 .49754429270011086 .5758942885575231)
Layer: #(.5629912680400353 .5629912680400353 .5629912680400353 .5629912680400353 .5629912680400353)
Layer: #(.5236843565380089 .5460310707424173 .5460310707424173 .5236843565380089)
Layer: #(.5514706684153556 .5514706684153556 .5514706684153556 .5514706684153556 .5514706684153556)
Layer: #(.568886012495078 .49268919231028063 .49268919231028063 .568886012495078)
;Value: #t

...

It's learning. The bug is... in my format command. How, what?!!
.

Start: Thu Oct 17 22:11:47 MDT 2013
Stop:  Thu Oct 17 22:22:55 MDT 2013

Going to quickly try to do a time trial with my new network...

Oooohhhhh baby... this is /beautiful/. I think the only more beautiful
performance comparison was the syntactic analyzer vs. the metacircular
scheme interpreter Alex and I wrote last year. Results:

   Compiler
Run time : 42470
GC Time  : 300
Real Time: 42961

    Matrix
Run time : 2810
GC Time  : 30
Real Time: 2846

An order of magnitude. Dag, yo. Win.
.

Start: Fri Oct 18 13:17:03 MDT 2013
Stop:  Fri Oct 18 13:50:53 MDT 2013

Character recognition works!! 1000 training cycles on the first for
letter bitmaps. Woot!!
.

Start: Tue Oct 22 08:00:11 MDT 2013
Stop:  Tue Oct 22 08:20:20 MDT 2013

Started work on board engine.
Notes: 0 means no piece. 1 means pawn. 2 means king.
Using bitmaps for white and black.
.

Start: Tue Oct 22 18:54:25 MDT 2013
Stop:  Tue Oct 22 19:14:11 MDT 2013
Start: Tue Oct 22 19:55:21 MDT 2013
Stop:  Tue Oct 22 20:31:30 MDT 2013

Continuing work on checkers engine.
Revised design: one bitmap, negative values represent black.
Still working on board printing.

Board prints correctly. (At least with non-kinged pieces)
.

Start: Wed Oct 23 07:44:49 MDT 2013
Stop:  Wed Oct 23 08:21:06 MDT 2013

Worked on asserting that a move is legal.
.

Start: Mon Oct 28 18:37:59 MDT 2013
Stop:  Mon Oct 28 19:03:23 MDT 2013
Start: Mon Oct 28 19:39:28 MDT 2013
Stop:  Mon Oct 28 19:48:27 MDT 2013

Going to try finishing move legality checking, etc.

This is what the board will look like:

     1   2   3   4   5   6   7   8
   +---+---+---+---+---+---+---+---+
 1 |   | w |   | w |   | w |   | w |
   +---+---+---+---+---+---+---+---+
 2 | w |   | w |   | w |   | w |   |
   +---+---+---+---+---+---+---+---+
 3 |   | w |   | w |   | w |   | w |
   +---+---+---+---+---+---+---+---+
 4 |   |   |   |   |   |   |   |   |
   +---+---+---+---+---+---+---+---+
 5 |   |   |   |   |   |   |   |   |
   +---+---+---+---+---+---+---+---+
 6 | b |   | b |   | b |   | b |   |
   +---+---+---+---+---+---+---+---+
 7 |   | b |   | b |   | b |   | b |
   +---+---+---+---+---+---+---+---+
 8 | b |   | b |   | b |   | b |   |
   +---+---+---+---+---+---+---+---+
.

Start: Tue Oct 29 20:05:03 MDT 2013
Stop:  Tue Oct 29 21:33:32 MDT 2013

I finished the first draft of the move making routines. Writing some
tests for that now...

Very basic tests in place. The last one is failing; I'm not sure
why. See the error message when engine_test.t is run.
.

Start: Wed Oct 30 07:45:43 MDT 2013
Stop:  Wed Oct 30 08:22:42 MDT 2013

Much movement seems to be working. See bug in engine_test.t: piece not
being removed from the board after being jumped.
Also todo: implement kinging, make sure that kings move properly, and
chained move legality checking (chains only legal if consecutive jumps.)
.

Start: Wed Oct 30 21:05:31 MDT 2013
Stop:  Wed Oct 30 21:12:23 MDT 2013
Start: Wed Oct 30 21:28:17 MDT 2013
Stop:  Wed Oct 30 21:45:16 MDT 2013

Pieces get kinged, fixed bug with captured piece not being removed.
.

Start: Thu Oct 31 18:58:07 MDT 2013
Stop:  Thu Oct 31 20:20:31 MDT 2013

Fixed some bugs with legal moves in my chess engine. I'm considering
starting on the search algorithm. I said I would use NegaScout, but
I've only worked with Negamax. I might want to start with that...

Also, it's kind of late, so I'm wondering if I should start something
like this. I think I'll go play the piano for a bit...
.

Start: Fri Nov  1 14:32:52 MDT 2013
Stop:  Fri Nov  1 14:42:02 MDT 2013
Start: Fri Nov  1 15:06:28 MDT 2013
Stop:  Fri Nov  1 16:37:44 MDT 2013

Going to start working on the negamax algorithm.

Working on move generation. I haven't tested any of the functions I've
written today (possible-moves, generate-possible-moves, and
collect-coordinates.) I'll need to write `collect-diagnal-squares',
which returns the 4 diagonal squares to the square given. I could (and
probably should) make this function efficient: have it do some /very/ basic
error checking: the direction is correct, and maybe if the square is
within bounds. This can't duplicate too much of the work of
assert-legal, otherwise I loose efficency.
.

Start: Mon Nov  4 20:44:27 MST 2013
Stop:  Mon Nov  4 21:56:26 MST 2013

Continuing work on move generators. It's late, so I'm not going to
spend too much time working on it.

I fixed a bug in collect-coordinates, which I'm sure improved the
efficenty of my program.

I'll need to profile this code at some point... First, I have to get
the thing working. That is the most important part: finish this
project!!
.

Start: Wed Nov  6 07:39:56 MST 2013
Stop:  Wed Nov  6 08:20:11 MST 2013

Worked on implementing jump trees. It's close, but not quite
there. The move generator is following the jump trees correctly, but
it's not returning the possibilities in the right format. I'll have to
go back and check all my CONSes, APPENDs, and LISTs.
.

Start: Thu Nov  7 20:54:03 MST 2013
Stop:  Thu Nov  7 21:23:20 MST 2013

Fixed broken jump chains. I'm only getting the maximum jump distance.
I need to find a way to generate sub-jump chains. I could call my
sub-jumps function, and then call something like remove-duplicates...
I don't know how efficient that would be though.
.

Start: Fri Nov  8 14:52:50 MST 2013
Stop:  Fri Nov  8 15:14:37 MST 2013

I've got sub jump chains returning now. I'm collecting the sub-chains
by calling sub-jumps and then passing the data to delete-duplicates.
.

Start: Fri Nov  8 19:17:28 MST 2013
Stop:  Fri Nov  8 22:26:45 MST 2013
Start: Fri Nov  8 22:50:52 MST 2013
Stop:  Fri Nov  8 23:33:36 MST 2013

This has been a fantastic session. I finished the first draft of my
Negamax algorithm. I've got it printing out diagnostics. D'Arvit that
thing is /fast/. 7 moves ahead in a 5-piece endgame in ~15 seconds.
Really good. I'm still trying to figure out if it is searching
correctly. I'm not sure how I'll test beta-cutoffs... that's going to
be hard.

I finished doing a 3-deep minimax algorithm by hand for testing
purposes. That wasn't fun. I'm glad I'm done now.

I'm working on debugging Negamax. This is kinda tricky...

Negamax is hard to debug because the game tree is so huge, and it's
difficult to insert any sort of debug hook into the search routine.

Something is not right with this algorithm. The worst part is that you
have to get negamax /exactly/ right or the entire thing blows up. No
error.

I'm thinking that given the bitmap:
  #(#(0 0 0 0) #(1 0 0 0) #(1 0 0 0) #(0 0 1 0) #(0 0 0 0) #(0 -1 -1 0) #(0 0 0 0) #(0 0 0 0))

Negamax should choose (32 43) (white to move) with a search depth of
3. I've worked out this game tree by hand, and I think it should
return a score of 2.

I need to go back through and make sure all my signs are right. In
particular, do I need to negate the score I get back from the static
evaluation?
.

Start: Sat Nov  9 19:59:39 MST 2013
Stop:  Sat Nov  9 22:17:30 MST 2013

This is desperately hard to debug. I've got negamax printing out a
nice tree-structure to help me visualize what it's doing. It looks
like it's evaluating a static position's score correctly... Wait!!!...

I'm not changing who's move it is when I call negamax from
best-moves-dumb. Let's see what that does...

IT GAVE ME THE RIGHT MOVE!! OK. I'm not quite sure if this is a
fluke. Now let's see how well it handles a game...

OK. It looks like I'm not returning the right value for a terminal
game state. Let's try debugging and tweaking that...

IT WORKS!! ATTENTION: NEGAMAX IS WORKING!!

OK. Now for a full game against me...
(aside: I've made the diagnostics look nicer now for game play)

TODO: Trap errors from engine for bad moves.
TODO: Make opening book or something.

I didn't get around to a full game... it took too long considering
its move at 7 search piles. I need to run a profiler on this thing to
see if I can up the performance any more.

IDEA: Maybe I can adjust the search piles throughout the course of the
game. I'll start out with a shallow search (3 to 5 piles) and then
gradually deepen the search. Also, I might need to make optimizations
that make it more agressive.
.

Start: Mon Nov 11 12:05:41 MST 2013
Stop: Mon Nov 11 12:09:04 MST 2013

Game tree search will pick move to end game, but other things (like
the player engine) do not work.
.

Start: Mon Nov 11 20:29:27 MST 2013
Stop:  Mon Nov 11 20:34:20 MST 2013

I did a little bit of clean up.
.

Start: Wed Nov 13 07:42:46 MST 2013
Stop:  Wed Nov 13 08:21:13 MST 2013

I've cleaned up the statistics that negamax shows as it walks the game
tree. I'd like to make sure that negamax is pruning branches correctly
(which I think it is) and I'd also like it to say something like "ten
moves to victory" like crafty. That'd be really fun.

Now I need to start training a neural network to do the work of
negamax. I'll replace the `score' function with a call to this neural
net. Hopefully, the net will give me something like 5 more search
piles.

I don't know how many hidden layers I will need. I guess I could talk
to Fred about that... I think I'll do that. I think I'll need two
layers.

TODO: Very important: make benchmarks.
.

Start: Thu Nov 14 19:40:03 MST 2013
Stop:  Thu Nov 14 21:06:37 MST 2013

I've made it so that the user can reset the negamax search depth while
playing the game.

I'd like to add the "ten moves to victory" or something, but that
might be hard, just because of how negamax is set up.

Wrote a benchmarks file, and created a directory to store
auto-generated benchmark results.

I had a bit of a problem with getting file handles to open in
Scheme... I forgot that I was closing the log file at the end of the
benchmarks file, so I kept getting error messages when I tried to
write to the file from the REPL. The error messages were not very
helpful... that's something that Perl is pretty good with. Scheme's
error messages are usually a bit more cryptic.
.

Start: Fri Nov 15 13:20:46 MST 2013
Stop:  Fri Nov 15 16:15:46 MST 2013

Getting some advice from Fred on how neural networks actually
work. See notes.org.

Reflections: Fred seems to think that neural networks are probably not
the best choice for what I'm doing. The biggest problem is that a
neural network is typically used for yes/no situations. I need to
extract a score from the neural network.

Fred gave me a referance to a corpus of training data that I can use
to make sure my neural networks are running correctly:
  http://archive.ics.uci.edu/ml/datasets/Iris

Really good book I'll have to check out: "Neural Smithing" by Russell
D. Reed and Robert J. Marks II.
.

Start: Mon Nov 18 21:06:58 MST 2013
Stop:

I need to make a list of features that my neural net should look
at.

Features:
  - number of white pawns and kings vs. number of black pawns and
    kings (2)
  - number of pawns within two moves of back row (2)
  - number of endangered pieces (2)
    + This will be good because it will mimic the quiescent score checker
  - number of possible moves (you and opponent) (2)
  - maximum number of consecutive empty rows (1)
  - number of opponent pieces in front of your pawns (2)
  - opponent's proximity to your pawns and kings (2)

So far, 13 inputs total.

I'll have to work to make these routines super-optimized.

TODO: find a way to use the data loaded from NN_DATA/network.scm with
my define-feed-forward-net macro. I might have to make a network, then
destructively modify the network with the stored values after I've
loaded them.

TODO: IMPORTANT!!! finish figuring out how to notate the network in
NN_DATA/network.scm
DO THIS FIRST!!

TODO: Add mechanism to automatically back up network states. Maybe
every time I load the network have it back up the file with the
timestamp.
.

Start: Tue Nov 19 11:58:34 MST 2013
Stop:  Tue Nov 19 12:05:07 MST 2013

Finished notating the network storage. Here's what the file looks
like: <<__NETWORK_SCM__
(0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0) ;outputs
((0 0 0 0 0 0 0 0 0 0 0 0 0)			      ;hidden layers
 (0 0 0 0 0 0 0 0 0 0 0 0 0)
 (0 0 0 0 0 0 0 0 0 0 0 0 0)
 (0 0 0 0 0 0 0 0 0 0 0 0 0)
 (0 0 0 0 0 0 0 0 0 0 0 0 0)
 (0 0 0 0 0 0 0 0 0 0 0 0 0)
 (0 0 0 0 0 0 0 0 0 0 0 0 0)
 (0 0 0 0 0 0 0 0 0 0 0 0 0)
 (0 0 0 0 0 0 0 0 0 0 0 0 0)
 (0 0 0 0 0 0 0 0 0 0 0 0 0)
 (0 0 0 0 0 0 0 0 0 0 0 0 0)
 (0 0 0 0 0 0 0 0 0 0 0 0 0)
 (0 0 0 0 0 0 0 0 0 0 0 0 0)
 (0 0 0 0 0 0 0 0 0 0 0 0 0)
 (0 0 0 0 0 0 0 0 0 0 0 0 0)
 (0 0 0 0 0 0 0 0 0 0 0 0 0)
 (0 0 0 0 0 0 0 0 0 0 0 0 0)
 (0 0 0 0 0 0 0 0 0 0 0 0 0)
 (0 0 0 0 0 0 0 0 0 0 0 0 0)
 (0 0 0 0 0 0 0 0 0 0 0 0 0)
 (0 0 0 0 0 0 0 0 0 0 0 0 0)
 (0 0 0 0 0 0 0 0 0 0 0 0 0)
 (0 0 0 0 0 0 0 0 0 0 0 0 0)
 (0 0 0 0 0 0 0 0 0 0 0 0 0)
 (0 0 0 0 0 0 0 0 0 0 0 0 0)
 (0 0 0 0 0 0 0 0 0 0 0 0 0))
__NETWORK_SCM__

Looks like lunch is just about over, so I need to go now.
.

Start: Wed Nov 20 07:45:00 MST 2013
Stop:  Wed Nov 20 08:21:50 MST 2013

Meta: TODO

Working on writing the feature detectors. I finished writing the code
to load the neural networks from file. I haven't tested this code yet,
however. Also, I need to write code to save the trained network and
back up the old network.

Oh... new problem: Exactly what values will I use to train the
network? Do I simply score the board with Negamax and then scale the
result between -1 and 1? That could work. I /think/ that will
work. I'll give it a shot.

Also, I need to get a server to do some heavy training.

I've implemented pawn and king count comparisons and advancement
potential. I'm currently working on counting the number of available
moves for a particular side (implemented) and the number of pieces in
danger of being jumped.
.

Start: Thu Nov 21 19:12:42 MST 2013
Stop:  Thu Nov 21 20:38:55 MST 2013

Meta: IMPROVEMENTS, TODO

I've finished the rough draft for the feature detectors. I'm going to
need to write some tests to make sure that this is really working.

I've got the basic feature detectors working! I don't know how
efficent they are; I'm sure I could up the performance on these
routines.

I might need to find more features about the board and implement
them. I've been thinking about the following:
  - how many enemy pieces are in front of my pieces
  - how many endangered kings and pawns (like the one I've got, except
    it separates kings and pawns)

TODO: I need to add more tests for jump chains. Here's some data to
help with testing:

     1   2   3   4   5   6   7   8
   +---+---+---+---+---+---+---+---+
 1 | . |   | . |   | . |   | . |   |
   +---+---+---+---+---+---+---+---+
 2 |   | . |   | . | B | . | b | . |
   +---+---+---+---+---+---+---+---+
 3 | . |   | . |   | . |   | . | w |
   +---+---+---+---+---+---+---+---+
 4 | w | . | b | . |   | . | b | . |
   +---+---+---+---+---+---+---+---+
 5 | . |   | . |   | . | b | . |   |
   +---+---+---+---+---+---+---+---+
 6 | b | . | b | . | w | . | W | . |
   +---+---+---+---+---+---+---+---+
 7 | . |   | . | b | . |   | . |   |
   +---+---+---+---+---+---+---+---+
 8 | b | . | b | . |   | . |   | . |
   +---+---+---+---+---+---+---+---+

#(#(0 0 0 0) #(0 0 -2 -1) #(0 0 0 1) #(1 -1 0 -1) #(0 0 -1 0) #(-1 -1 1 2) #(0 -1 0 0) #(-1 -1 0 0))


     1   2   3   4   5   6   7   8
   +---+---+---+---+---+---+---+---+
 1 | . |   | . |   | . |   | . |   |
   +---+---+---+---+---+---+---+---+
 2 | w | . |   | . |   | . | w | . |
   +---+---+---+---+---+---+---+---+
 3 | . | b | . |   | . | B | . | w |
   +---+---+---+---+---+---+---+---+
 4 | w | . |   | . | b | . |   | . |
   +---+---+---+---+---+---+---+---+
 5 | . | w | . |   | . | b | . | b |
   +---+---+---+---+---+---+---+---+
 6 | b | . | b | . |   | . |   | . |
   +---+---+---+---+---+---+---+---+
 7 | . | b | . | b | . | W | . |   |
   +---+---+---+---+---+---+---+---+
 8 | b | . | b | . |   | . |   | . |
   +---+---+---+---+---+---+---+---+

#(#(0 0 0 0) #(1 0 0 1) #(-1 0 -2 1) #(1 0 -1 0) #(1 0 -1 -1) #(-1 -1 0 0) #(-1 -1 2 0) #(-1 -1 0 0))

     1   2   3   4   5   6   7   8
   +---+---+---+---+---+---+---+---+
 1 | . |   | . |   | . |   | . |   |
   +---+---+---+---+---+---+---+---+
 2 | w | . |   | . | B | . | w | . |
   +---+---+---+---+---+---+---+---+
 3 | . |   | . |   | . | w | . | w |
   +---+---+---+---+---+---+---+---+
 4 | w | . | w | . | b | . | w | . |
   +---+---+---+---+---+---+---+---+
 5 | . | w | . | b | . |   | . | b |
   +---+---+---+---+---+---+---+---+
 6 | b | . | b | . |   | . | b | . |
   +---+---+---+---+---+---+---+---+
 7 | . | b | . | b | . | b | . |   |
   +---+---+---+---+---+---+---+---+
 8 | b | . | b | . |   | . |   | . |
   +---+---+---+---+---+---+---+---+

#(#(0 0 0 0) #(1 0 -2 1) #(0 0 1 1) #(1 1 -1 1) #(1 -1 0 -1) #(-1 -1 0 -1) #(-1 -1 -1 0) #(-1 -1 0 0))

That's all for tonight.
.

Start: Fri Nov 22 14:23:12 MST 2013
Stop:  Fri Nov 22 15:02:00 MST 2013

I need to encode the rest of the tests today, as well as impelement
some new feature detectors.

Another extremely important feature: who's turn it is to move.

Oh fie. How many output nodes should I have? I'll need something to
sort moves by. I guess I could have it return a score: 1 good for
white, -1 good for black, etc. I'm not quite sure how well training
that will go. I'll give it a shot.

As to the number of nodes... I'm not quite sure.
.

Start: Sat Nov 23 19:46:42 MST 2013
Stop:  Sat Nov 23 21:52:53 MST 2013

Meta: TODO

I'm going to work on setting up the network tonight.

I've got a new feature detector working; I wrote a function to tell me
how many enemy pieces are ahead of the furthest one of my pieces. This
obviously doesn't tell me everything, but it can tell me a bit about
the board.

I've been considering using self-training networks to make this neural
network. However, I've already got the backprop network working well,
so I'll try using that.

TODO: Write tests using the data that Fred pointed me to to make sure
that back prop is working perfectly.

I reorg'd a bit of the engines/checkers/ directory: I'm putting the
feature detectors into their own file.

OK, I think I've found a better network configuration. Here's the
notes and the starting network:

Inputs: 11
  - pawn-compare
  - king-compare
  - white-advancement-potential
  - black-advancement-potential
  - movement analysis (2)
  - endanger analysis (2)
  - pieces-ahead-of-white
  - pieces-ahead-of-black
  - current-turn
Outputs: 5
  - very good for white
  - good for white
  - whatever
  - good for black
  - very good for black

<<__NETWORK__
((0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0)  ; output nodes
 (0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0)
 (0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0)
 (0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0)
 (0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0))
((0 0 0 0 0 0 0 0 0 0 0)                        ; hidden nodes
 (0 0 0 0 0 0 0 0 0 0 0)
 (0 0 0 0 0 0 0 0 0 0 0)
 (0 0 0 0 0 0 0 0 0 0 0)
 (0 0 0 0 0 0 0 0 0 0 0)
 (0 0 0 0 0 0 0 0 0 0 0)
 (0 0 0 0 0 0 0 0 0 0 0)
 (0 0 0 0 0 0 0 0 0 0 0)
 (0 0 0 0 0 0 0 0 0 0 0)
 (0 0 0 0 0 0 0 0 0 0 0)
 (0 0 0 0 0 0 0 0 0 0 0)
 (0 0 0 0 0 0 0 0 0 0 0)
 (0 0 0 0 0 0 0 0 0 0 0)
 (0 0 0 0 0 0 0 0 0 0 0)
 (0 0 0 0 0 0 0 0 0 0 0)
 (0 0 0 0 0 0 0 0 0 0 0)
 (0 0 0 0 0 0 0 0 0 0 0)
 (0 0 0 0 0 0 0 0 0 0 0)
 (0 0 0 0 0 0 0 0 0 0 0)
 (0 0 0 0 0 0 0 0 0 0 0)
 (0 0 0 0 0 0 0 0 0 0 0)
 (0 0 0 0 0 0 0 0 0 0 0))
.

Start: Mon Nov 25 18:45:36 MST 2013
Stop:  Mon Nov 25 19:15:08 MST 2013

Apropos the outputs, I'm not quite sure what my thresholds should be
for training. I'll start with the following, but I'll make sure to be
open to experimentation.
  Score:
    10..1000   - good (.2), very good (1)
    6..10      - good (.7), very good (.7)
    1..6       - whatever (.2), good (1), very good (.2)
    0          - bad (.2), whatever (1), good (.2)
    -1..-6     - whatever (.2), bad (1), very bad (.2)
    -6..-10    - bad (.7), very bad (.7)
    -10..-1000 - bad (.2), very bad (1)
.

Start: Wed Nov 27 16:23:14 MST 2013
Stop:  Wed Nov 27 18:17:40 MST 2013

Today I want to get the training framework set up. I'll need to make
it flexible so I can modify it later. First I'll need to enumerate
what I'll want to tweak:
  - where to load and save the training data
  - number of hidden-layer nodes
  - potentially, the number of imputs
  - number of outputs
  - scores assocciated with outputs (see previous set of notes)

I'll need to make some macros to handle this stuff.

***

I decided to work on performing some really rigorous tests on the
neural networks. I haven't finished these, but I've got the data
formatted and everything.

I'm also working on a training library. The function has yet to be
completely implemented, but it will partition the training data you
give it into some number of specified sets, train with all but one of
the sets, and then validate with the remaining set. Rotate through
training with sets until the validation error is below a certain
threshold. (All configurable.)

FOR NEXT TIME: Finish lib/network_trainer.scm
.

Start: Thu Nov 28 15:01:40 MST 2013
Stop:  Thu Nov 28 15:31:06 MST 2013
Start: Thu Nov 28 19:19:39 MST 2013
Stop:  Thu Nov 28 21:09:24 MST 2013

Did some more work on lib/network_trainer.scm. It's still not
finished, but it's a lot closer. I just need to write the function
that uses the validation set.

Finished rough draft of lib/network_trainer.scm. I need to debug this
now...

Alright! I'm pretty sure it's working. I've been using it to aid in
the heavy_testing/ file, and it's training properly. It's fairly nice
to use. I'm happy with it. However, there /is/ one problem: it doesn't
support on-the-fly generated training data. I might be able to use
lazy evaluation to do what I want (which would be really cool!) but
I'm not quite sure... Lazy streams don't exactly lend themselves to be
shuffled. However, if I'm continuously generating new board moves (how
the heck am I going to do that anyways?) I might not need to shuffle
the board. I should probably save every board state and the
coresponding score.

OH! I thought of another feature: how many pieces are simultaniously
in danger. (Translation: what is the longest possible jump for the
other side?)

I had one problem where the flower-net in heavy_testing/ didn't train
correctly. However, I've run the training framework on the net maybe 7
times and haven't had a single problem. I'll keep an eye on it, but it
looks like my neural network engine is good.
.

Start: Sat Nov 30 19:16:28 MST 2013
Stop:  Sat Nov 30 22:58:17 MST 2013
Additions: 1:00 (thinking time)
Subtractions: 1:00 (I probably spent this much time helping my mom and
generally loafing)

I finally figured out how I'm going to generate random moves! It'll
take some doing... First, I need to add a feature for longest possible
jump. That'll be what I do first, then I'll code the data generators
and start one off on the vm. (Oh, I got a vm from Better Servers by
the way.)

***

Alright! I've finished the rough draft for my game-tree wanderer. I'm
going to commit this and then clone my repo onto my vm to test the
game-tree wanderer algorithm/code.

I'm generating training data!! (I'm doing this on the vm. Woot!!)
Hopefully my program is working correctly.
.

Start: Tue Dec  3 07:41:11 MST 2013
Stop:  Tue Dec  3 08:22:14 MST 2013

My data generator is not working; I'm going to try rewriting it.

I got a memory overflow error. >:-( This is going to be tricky to debug.

Second draft done. Need to debug.
.

Start: Tue Dec  3 19:10:19 MST 2013
Stop:  Tue Dec  3 21:26:26 MST 2013
Subtractions: 0:45

I wonder if the memory leak is in negamax... I'll take a look.

Hhmmm... looks like it might be. I'm going to check again.

OK, I know it's not possible-moves, which is a releif. That seems to
have its memory reclaimed when the garbage collector runs. I suppose
it /could/ still be this function when another function is calling it
or what not, but I don't think so.

I take that back. `negamax' seems to have the memory it uses reclaimed
just fine. Curious...

I checked the basic board movement routines (possible-moves, do-move,
etc.) and they don't seem to have any memory leaks. :-( I'm
stumped. I'll look at my data generating code again.

OK! I don't think it was a memory leak, actually... I'm guessing that
it was that 1000-position cache that I kept in memory. I've reduced
the cache to 20 positions, and it seems to be running just fine. I'd
like to get this up to at least 100 so I get lots of interesting
positions. There's a command-line flag (--heap (see the mit-scheme
user manual (https://www.gnu.org/software/mit-scheme/documentation/mit-scheme-user/Command_002dLine-Options.html#Command_002dLine-Options)))

that will reset the amount of heap space that scheme will use. I'll
need to multiply the current value (whatever that is (I need to find
this out somehow (I think print-gc-statistics might tell me the number
I need (I've been programming in Scheme for too long (can you
tell?))))) by at least 5. I wonder how much memory I can allocate...

TODO: Find out how many 1024-word blocks mit-scheme has as heap space
by default, then multiply this value by at least 5.
.

Start: Wed Dec  4 07:56:51 MST 2013
Stop:  Wed Dec  4 08:20:00 MST 2013

Going to try increasing the memory available to scheme via the command
line. First I need to figure out how much heap space it has by
default...
.

Start: Wed Dec  4 20:53:14 MST 2013
Stop:  Wed Dec  4 21:53:10 MST 2013

Fie!! I ran out of memory /again/!!! Statistics:
  search-depth:      7
  pool-size:       100
  heap-size:    115941

Still not enough! Maybe I should drop search-depth... I'm going to
profile negamax to see if I can figure out if something's wrong.

OK, I tried profiling my code, but I need to compile it first. Here I
go!!

Dag, yo. This is more work than I thought. I'm going to have to spend
some time reorganizing my entire project. That's going to be... ugly.
.

Start: Thu Dec  5 21:08:06 MST 2013
Stop:  Thu Dec  5 22:20:21 MST 2013

I'm going to try reorganzing my project's directory heirarchy. I'm
going to back up my changes to so the commit is stable, then reorg
completely. Wish me luck!!

OK! Swanky new structure is in place. Everything is broken right now;
I'm working on fixing that. I still need to figure out how to compile
files that require other files...

I figured out how to exit MIT-Scheme without having any prompt for
confirmation: use the function `%exit'. 'Tis a hidden function. I
found it! The secret way... into MORDOR!!

*ahem*

Well, `make' is going to be lovely. This is the first project I've
needed it this badly. I've used it once before, and it was nice. This
time, it is going to be vital.

One more note: I'm dumping everything from STDOUT to /dev/null so I
don't get all these messy compile messages from mit-scheme as I
repeatedly call it from the command line. I guess I should print the
data while I debug my compiling and makefile, and then pipe it to
/dev/null once I'm done with that.

Good night.
.

Start: Fri Dec  6 12:30:30 MST 2013
Stop:  Fri Dec  6 12:50:11 MST 2013

I've got a bit of time here at school, so I'm going to keep working on
my project. I've added to .gitignore the files that are generated
during scheme compilation.
.
