Start: Wed Sep 11 08:00:02 MDT 2013
Stop:  Wed Sep 11 08:20:49 MDT 2013
Total: ~0:20

This is to help me keep track of the project, and so that I don't have
to clutter up the git commit with these longer entries.

Today I'm working on implementing the backprop algorithm. I'm using
that PDF series I found. Once I get that working I'll write some tests
using the examples in the PDF.
.

Start: Thu Sep 12 17:29:03 MDT 2013
Stop:  Thu Sep 12 20:32:24 MDT 2013
Subtractions: 1:00
Total: ~2:00

Finished writing a crude back propogation algorithm! The interface is
very messy, but it passed the tests. Now that I've got the very basics
of the network working, I'll try my network model out on some more
complicated examples, and test doing "real" learning with it. For fun,
I could teach it to play tic-tac-toe.

I'm wanting to rewrite the network simulator. The model I've got right
now is kinda messy and probably fairly inefficent. I think I'll try
writing another closure-based network system, but this time do it
better.

Eventually, I'd like to write the network with closures that compile
into some nice, fast code. I might wait to do this until after I've
got other elements of this project working, such as the chess engine
and the Negascout framework.
.

Start: Sat Sep 14 17:42:30 MDT 2013
Stop:  Sat Sep 14 20:34:51 MDT 2013
Total: ~3:00

I didn't like how ugly the closure nodes looked, so I started working
on a version that uses scheme macros. To do this, I picked a syntax
for the networks that I liked better than the old syntax, and then
basically wrote my own Domain Specific Language to implement the new
syntax.

I haven't ever done much work with Scheme's pattern matching
macros. I've used Common Lisp's macros extensivly, and thanks to Paul
Graham's book "On Lisp", I got to the point where I wrote an extremely
complex macro system that allowed me to write a very concise natural
language parser. (By concise I mean it had minimal boiler plate and
was easy to read.) The macros I'm writing for this are the largest
I've written to date (in Scheme of course), but they are far from the
most complex out there.
.

Start: Sat Sep 14 21:17:45 MDT 2013
Stop:  Sat Sep 14 23:25:43 MDT 2013
Subtractions: 0:10
Total: ~2:00

I got back propagation working with my macro tests!! For one day, I've
done a lot. I'm going to go to bed now...
.

Start: Mon Sep 16 20:55:00 MDT 2013
Stop:  Mon Sep 16 22:00:47 MDT 2013
Total: 1:05

Working on doing character recognition. Not going so well... I don't
know what's wrong. I need to debug this before I move onto my chess
engines etc.
.

Start: Tue Sep 17 19:41:31 MDT 2013
Stop:  Tue Sep 17 20:57:54 MDT 2013
Subractions: 0:00
Total: ~1:00

Debugging the character recognition net. I wrote some simpler pattern
learning networks, which worked. I'm going to get a server and train
my character recognizer heavily.
.

Start: Wed Sep 18 08:08:18 MDT 2013
Stop:  Wed Sep 18 08:21:49 MDT 2013
Total: 0:13

Worked on compiling the net into closures.
.

Start: Thu Sep 19 19:16:39 MDT 2013
Stop:  Thu Sep 19 19:21:22 MDT 2013
Start: Thu Sep 19 20:32:48 MDT 2013
Stop:  Thu Sep 19 22:19:47 MDT 2013
Total: ~1:45

Worked on debugging the new system. I've fixed quite a few minor bugs,
but I have no idea as to how many I've got left. Right now the values
for nodes (at least the output nodes) are not being set
correctly. Obviously, this is a problem as I query those nodes to get
the output of the neural net. I'm suspecting the problem is where I
have the logic for getting node values. (See the closure construction
of the nodes.)
.

Start: Fri Sep 20 20:03:01 MDT 2013
Stop:  Fri Sep 20 21:45:30 MDT 2013

Major debug session. Still not working. I'm absolutly mystified with a
particular problem: either I'm missing a very subtle bug with my usage
of `case', or the `case' macro (or even worse, something even more
low-level) has a bug in it. The bug appeared when I added a `let'
arround the place where I'm using `case'... I'm considering replacing
my usage of `case' with a `cond' expression so I can poke around in
the internals a bit more.
.

Start: Sat Sep 21 15:55:32 MDT 2013
Stop:  Sat Sep 21 17:38:02 MDT 2013

Got compiling into closures working. I need to do a speed trial, and
try out my character recognition with the new system.

That was one heck of a debug session!
.

Start: Sat Sep 21 20:51:32 MDT 2013
Stop:  Sat Sep 21 21:40:02 MDT 2013

Going to do some profiling.

...

I am absolutely mystified as to why on earth the new compiled macros
are running slower than the old ones. ?!! I need a profiler. Bad.
.

Start: Tue Sep 24 17:57:57 MDT 2013
Stop:  Tue Sep 24 19:10:03 MDT 2013

I'm going to try writing my feed-forward networks with matricies. This
should be pretty dang fast if I can code it right.
.

Start: Wed Sep 25 07:40:15 MDT 2013
Stop:  Wed Sep 25 08:20:56 MDT 2013

More work on ff-networks with matricies. There are no native matricies
for MIT Scheme, so I have to implement them myself with the native
vectors.
.

Start: Thu Sep 26 18:33:58 MDT 2013
Stop:  Thu Sep 26 20:02:00 MDT 2013

Working on implementing the backprop algorithm with matricies.
.

Start: Mon Sep 30 16:12:20 MDT 2013
Stop:  Mon Sep 30 19:17:13 MDT 2013

Ditto.

I'm still getting some errors. My calculation of the value
"forward-errors" on line 152 does not match the correct
algorithm. Compare with (δαWAα + δβWAβ) and friends on page 19 of
chapter 3 of the neural net tutorial PDFs.

I'm too tired right now to figure out the correct matrix
computations.
.

Start: Tue Oct  1 20:28:32 MDT 2013
Stop:  Tue Oct  1 21:08:36 MDT 2013
Start: Tue Oct  1 21:19:18 MDT 2013
Stop:  Tue Oct  1 21:24:52 MDT 2013

Still working on backprop algorithm.

IT'S WORKING!!! I think...

Nope. Something is still wrong.
.

Start: Wed Oct  2 07:42:59 MDT 2013
Stop:  Wed Oct  2 08:22:21 MDT 2013
Start: Wed Oct  2 09:26:32 MDT 2013
Stop:  Wed Oct  2 10:07:28 MDT 2013

It /looks/ like the mathematics are correct, but for some reason the
numbers are off after about 3 decimal places. Still not learning.

I think the decimal place bug is due to I'm working at a much higher
precision.
.

Start: Wed Oct  2 13:15:21 MDT 2013
Stop:  Wed Oct  2 13:21:34 MDT 2013
Start: Wed Oct  2 17:49:16 MDT 2013
Stop:  Wed Oct  2 18:52:55 MDT 2013

Rewriting the back prop algorithm. The code is a /tremendous/ amount
cleaner than the last one. See the "BUG:" note for my current
problem.
.

Start: Wed Oct 16 18:38:31 MDT 2013
Stop:  Wed Oct 16 18:50:31 MDT 2013
Start: Wed Oct 16 20:45:54 MDT 2013
Stop:  Wed Oct 16 23:21:17 MDT 2013

Recommencing work on progress after a lengthy break.

I've got learning with matricies working... sort of. The network is
only learning the first pattern given to it, which makes me think that
the error is caused by me not updating the values or the targets of
nodes somewhere...
I did some more checking, and I got different results from what I was
expecting: I trained the net once right at the start with pattern 1.
Then I trained it 1000 times with pattern 0. I expected that the
net would kinda learn pattern 1, and not learn pattern 0. However, the
network learned pattern 0, but returned roughly the same result for
pattern 1. I still think it's side effects.

I HAVE DONE SOMETHING VERY WRONG WITH MY DATA STRUCTURES!! Two x-rays:

Before /any/ activity:
1 ]=> (test-net01 'x-ray)
-------------- FULL DUMP FOR: 'test-net01' --------------
LAYER DUMP:
#(.1 .1 .1 .1)#(.1 .1 .1 .1)#(.1 .1 .1 .1)#(.1 .1 .1 .1)#(.1 .1 .1 .1)
#(#(.1 .1 .1 .1 .1) #(.1 .1 .1 .1 .1) #(.1 .1 .1 .1 .1) #(.1 .1 .1 .1
.1))
ERROR DUMP:
#(0 0 0 0)#(0 0 0 0)#(0 0 0 0)#(0 0 0 0)#(0 0 0 0)
#(0 0 0 0)
VALUE DUMP:
#(0 0 0 0)#(0 0 0 0)#(0 0 0 0)#(0 0 0 0)#(0 0 0 0)
#(#(0 0 0 0 0) #(0 0 0 0 0) #(0 0 0 0 0) #(0 0 0 0 0))
Reverse lists are up-to-date.
;Unspecified return value

1 ]=> (train-01-0 1)
;Value: #t

1 ]=> (test-net01 'x-ray)
-------------- FULL DUMP FOR: 'test-net01' --------------
LAYER DUMP:
#(.10668521032893902 .1 .1 .10668521032893902)#(.10668521032893902 .1
.1 .10668521032893902)#(.10668521032893902 .1 .1
.10668521032893902)#(.10668521032893902 .1 .1
.10668521032893902)#(.10668521032893902 .1 .1 .10668521032893902)
#(#(.15823362381053097 .15823362381053097 .15823362381053097
.15823362381053097 .15823362381053097)
  #(2.3340011090897797e-2 2.3340011090897797e-2 2.3340011090897797e-2
  2.3340011090897797e-2 2.3340011090897797e-2)
  #(2.3340011090897797e-2 2.3340011090897797e-2 2.3340011090897797e-2
  2.3340011090897797e-2 2.3340011090897797e-2)
  #(.15823362381053097 .15823362381053097 .15823362381053097
  .15823362381053097 .15823362381053097))
ERROR DUMP:
6.685210328939012e-36.685210328939012e-36.685210328939012e-36.685210328939012e-36.685210328939012e-3
#(.10591128248738689 -.13942387935960118 -.13942387935960118
.10591128248738689)
VALUE DUMP:
.549833997312478.549833997312478.549833997312478.549833997312478.549833997312478
#(.5682996204455919 .5682996204455919 .5682996204455919
.5682996204455919)
Reverse lists are up-to-date.
;Unspecified return value

Notice how before any activity, the values of _a_ layer are held in a
list of arrays. But after training, the values of _a_ layer are held
in a flat list! (Also note how they are all the same: this, I think is
very wrong. (Maybe not for the first training session... or maybe
yes. I don't know.)) Values should be a flat list, and errors need to
be a list of arrays.

OK, this is interesting. Every other training session, the layer
values are different from one another! Hhmmm...

1 ]=> (train-01 10)
Layer: #(.549833997312478 .549833997312478 .549833997312478 .549833997312478 .549833997312478)
Layer: #(.5682996204455919 .5682996204455919 .5682996204455919 .5682996204455919)
Layer: #(.549833997312478 .549833997312478 .549833997312478 .549833997312478 .549833997312478)
Layer: #(.6070696428491587 .5160359129077615 .5160359129077615 .6070696428491587)
Layer: #(.5531411451842421 .5531411451842421 .5531411451842421 .5531411451842421 .5531411451842421)
Layer: #(.5541421639397078 .5617705501752163 .5617705501752163 .5541421639397078)
Layer: #(.5497040086343132 .5497040086343132 .5497040086343132 .5497040086343132 .5497040086343132)
Layer: #(.5947277487620963 .5091361718820695 .5091361718820695 .5947277487620963)
Layer: #(.5564459877924409 .5564459877924409 .5564459877924409 .5564459877924409 .5564459877924409)
Layer: #(.5421537279531315 .5559192168507334 .5559192168507334 .5421537279531315)
Layer: #(.5499970779641044 .5499970779641044 .5499970779641044 .5499970779641044 .5499970779641044)
Layer: #(.5844245663272676 .5030009403207444 .5030009403207444 .5844245663272676)
Layer: #(.5597310060673503 .5597310060673503 .5597310060673503 .5597310060673503 .5597310060673503)
Layer: #(.532084739142932 .5506905464203923 .5506905464203923 .532084739142932)
Layer: #(.5506131434950248 .5506131434950248 .5506131434950248 .5506131434950248 .5506131434950248)
Layer: #(.5758942885575231 .49754429270011086 .49754429270011086 .5758942885575231)
Layer: #(.5629912680400353 .5629912680400353 .5629912680400353 .5629912680400353 .5629912680400353)
Layer: #(.5236843565380089 .5460310707424173 .5460310707424173 .5236843565380089)
Layer: #(.5514706684153556 .5514706684153556 .5514706684153556 .5514706684153556 .5514706684153556)
Layer: #(.568886012495078 .49268919231028063 .49268919231028063 .568886012495078)
;Value: #t

...

It's learning. The bug is... in my format command. How, what?!!
.

Start: Thu Oct 17 22:11:47 MDT 2013
Stop:  Thu Oct 17 22:22:55 MDT 2013

Going to quickly try to do a time trial with my new network...

Oooohhhhh baby... this is /beautiful/. I think the only more beautiful
performance comparison was the syntactic analyzer vs. the metacircular
scheme interpreter Alex and I wrote last year. Results:

   Compiler
Run time : 42470
GC Time  : 300
Real Time: 42961

    Matrix
Run time : 2810
GC Time  : 30
Real Time: 2846

An order of magnitude. Dag, yo. Win.
.

Start: Fri Oct 18 13:17:03 MDT 2013
Stop:  Fri Oct 18 13:50:53 MDT 2013

Character recognition works!! 1000 training cycles on the first for
letter bitmaps. Woot!!
.

Start: Tue Oct 22 08:00:11 MDT 2013
Stop:  Tue Oct 22 08:20:20 MDT 2013

Started work on board engine.
Notes: 0 means no piece. 1 means pawn. 2 means king.
Using bitmaps for white and black.
.

Start: Tue Oct 22 18:54:25 MDT 2013
Stop:  Tue Oct 22 19:14:11 MDT 2013
Start: Tue Oct 22 19:55:21 MDT 2013
Stop:  Tue Oct 22 20:31:30 MDT 2013

Continuing work on checkers engine.
Revised design: one bitmap, negative values represent black.
Still working on board printing.

Board prints correctly. (At least with non-kinged pieces)
.

Start: Wed Oct 23 07:44:49 MDT 2013
Stop:  Wed Oct 23 08:21:06 MDT 2013

Worked on asserting that a move is legal.
.

Start: Mon Oct 28 18:37:59 MDT 2013
Stop:  Mon Oct 28 19:03:23 MDT 2013
Start: Mon Oct 28 19:39:28 MDT 2013
Stop:  Mon Oct 28 19:48:27 MDT 2013

Going to try finishing move legality checking, etc.

This is what the board will look like:

     1   2   3   4   5   6   7   8
   +---+---+---+---+---+---+---+---+
 1 |   | w |   | w |   | w |   | w |
   +---+---+---+---+---+---+---+---+
 2 | w |   | w |   | w |   | w |   |
   +---+---+---+---+---+---+---+---+
 3 |   | w |   | w |   | w |   | w |
   +---+---+---+---+---+---+---+---+
 4 |   |   |   |   |   |   |   |   |
   +---+---+---+---+---+---+---+---+
 5 |   |   |   |   |   |   |   |   |
   +---+---+---+---+---+---+---+---+
 6 | b |   | b |   | b |   | b |   |
   +---+---+---+---+---+---+---+---+
 7 |   | b |   | b |   | b |   | b |
   +---+---+---+---+---+---+---+---+
 8 | b |   | b |   | b |   | b |   |
   +---+---+---+---+---+---+---+---+
.

Start: Tue Oct 29 20:05:03 MDT 2013
Stop:  Tue Oct 29 21:33:32 MDT 2013

I finished the first draft of the move making routines. Writing some
tests for that now...

Very basic tests in place. The last one is failing; I'm not sure
why. See the error message when engine_test.t is run.
.

Start: Wed Oct 30 07:45:43 MDT 2013
Stop:  Wed Oct 30 08:22:42 MDT 2013

Much movement seems to be working. See bug in engine_test.t: piece not
being removed from the board after being jumped.
Also todo: implement kinging, make sure that kings move properly, and
chained move legality checking (chains only legal if consecutive jumps.)
.

Start: Wed Oct 30 21:05:31 MDT 2013
Stop:  Wed Oct 30 21:12:23 MDT 2013
Start: Wed Oct 30 21:28:17 MDT 2013
Stop:  Wed Oct 30 21:45:16 MDT 2013

Pieces get kinged, fixed bug with captured piece not being removed.
.

Start: Thu Oct 31 18:58:07 MDT 2013
Stop:  Thu Oct 31 20:20:31 MDT 2013

Fixed some bugs with legal moves in my chess engine. I'm considering
starting on the search algorithm. I said I would use NegaScout, but
I've only worked with Negamax. I might want to start with that...

Also, it's kind of late, so I'm wondering if I should start something
like this. I think I'll go play the piano for a bit...
.

Start: Fri Nov  1 14:32:52 MDT 2013
Stop:  Fri Nov  1 14:42:02 MDT 2013
Start: Fri Nov  1 15:06:28 MDT 2013
Stop:  Fri Nov  1 16:37:44 MDT 2013

Going to start working on the negamax algorithm.

Working on move generation. I haven't tested any of the functions I've
written today (possible-moves, generate-possible-moves, and
collect-coordinates.) I'll need to write `collect-diagnal-squares',
which returns the 4 diagonal squares to the square given. I could (and
probably should) make this function efficient: have it do some /very/ basic
error checking: the direction is correct, and maybe if the square is
within bounds. This can't duplicate too much of the work of
assert-legal, otherwise I loose efficency.
.

Start: Mon Nov  4 20:44:27 MST 2013
Stop:  Mon Nov  4 21:56:26 MST 2013

Continuing work on move generators. It's late, so I'm not going to
spend too much time working on it.

I fixed a bug in collect-coordinates, which I'm sure improved the
efficenty of my program.

I'll need to profile this code at some point... First, I have to get
the thing working. That is the most important part: finish this
project!!
.

Start: Wed Nov  6 07:39:56 MST 2013
Stop:  Wed Nov  6 08:20:11 MST 2013

Worked on implementing jump trees. It's close, but not quite
there. The move generator is following the jump trees correctly, but
it's not returning the possibilities in the right format. I'll have to
go back and check all my CONSes, APPENDs, and LISTs.
.

Start: Thu Nov  7 20:54:03 MST 2013
Stop:  Thu Nov  7 21:23:20 MST 2013

Fixed broken jump chains. I'm only getting the maximum jump distance.
I need to find a way to generate sub-jump chains. I could call my
sub-jumps function, and then call something like remove-duplicates...
I don't know how efficient that would be though.
.

Start: Fri Nov  8 14:52:50 MST 2013
Stop:  Fri Nov  8 15:14:37 MST 2013

I've got sub jump chains returning now. I'm collecting the sub-chains
by calling sub-jumps and then passing the data to delete-duplicates.
.

Start: Fri Nov  8 19:17:28 MST 2013
Stop:  Fri Nov  8 22:26:45 MST 2013
Start: Fri Nov  8 22:50:52 MST 2013
Stop:  Fri Nov  8 23:33:36 MST 2013

This has been a fantastic session. I finished the first draft of my
Negamax algorithm. I've got it printing out diagnostics. D'Arvit that
thing is /fast/. 7 moves ahead in a 5-piece endgame in ~15 seconds.
Really good. I'm still trying to figure out if it is searching
correctly. I'm not sure how I'll test beta-cutoffs... that's going to
be hard.

I finished doing a 3-deep minimax algorithm by hand for testing
purposes. That wasn't fun. I'm glad I'm done now.

I'm working on debugging Negamax. This is kinda tricky...

Negamax is hard to debug because the game tree is so huge, and it's
difficult to insert any sort of debug hook into the search routine.

Something is not right with this algorithm. The worst part is that you
have to get negamax /exactly/ right or the entire thing blows up. No
error.

I'm thinking that given the bitmap:
  #(#(0 0 0 0) #(1 0 0 0) #(1 0 0 0) #(0 0 1 0) #(0 0 0 0) #(0 -1 -1 0) #(0 0 0 0) #(0 0 0 0))

Negamax should choose (32 43) (white to move) with a search depth of
3. I've worked out this game tree by hand, and I think it should
return a score of 2.

I need to go back through and make sure all my signs are right. In
particular, do I need to negate the score I get back from the static
evaluation?
.

Start: Sat Nov  9 19:59:39 MST 2013
Stop:  Sat Nov  9 22:17:30 MST 2013

This is desperately hard to debug. I've got negamax printing out a
nice tree-structure to help me visualize what it's doing. It looks
like it's evaluating a static position's score correctly... Wait!!!...

I'm not changing who's move it is when I call negamax from
best-moves-dumb. Let's see what that does...

IT GAVE ME THE RIGHT MOVE!! OK. I'm not quite sure if this is a
fluke. Now let's see how well it handles a game...

OK. It looks like I'm not returning the right value for a terminal
game state. Let's try debugging and tweaking that...

IT WORKS!! ATTENTION: NEGAMAX IS WORKING!!

OK. Now for a full game against me...
(aside: I've made the diagnostics look nicer now for game play)

TODO: Trap errors from engine for bad moves.
TODO: Make opening book or something.

I didn't get around to a full game... it took too long considering
its move at 7 search piles. I need to run a profiler on this thing to
see if I can up the performance any more.

IDEA: Maybe I can adjust the search piles throughout the course of the
game. I'll start out with a shallow search (3 to 5 piles) and then
gradually deepen the search. Also, I might need to make optimizations
that make it more agressive.
.

Start: Mon Nov 11 12:05:41 MST 2013
Stop: Mon Nov 11 12:09:04 MST 2013

Game tree search will pick move to end game, but other things (like
the player engine) do not work.
.

Start: Mon Nov 11 20:29:27 MST 2013
Stop:  Mon Nov 11 20:34:20 MST 2013

I did a little bit of clean up.
.

Start: Wed Nov 13 07:42:46 MST 2013
Stop:  Wed Nov 13 08:21:13 MST 2013

I've cleaned up the statistics that negamax shows as it walks the game
tree. I'd like to make sure that negamax is pruning branches correctly
(which I think it is) and I'd also like it to say something like "ten
moves to victory" like crafty. That'd be really fun.

Now I need to start training a neural network to do the work of
negamax. I'll replace the `score' function with a call to this neural
net. Hopefully, the net will give me something like 5 more search
piles.

I don't know how many hidden layers I will need. I guess I could talk
to Fred about that... I think I'll do that. I think I'll need two
layers.

TODO: Very important: make benchmarks.
.

Start: Thu Nov 14 19:40:03 MST 2013
Stop:  Thu Nov 14 21:06:37 MST 2013

I've made it so that the user can reset the negamax search depth while
playing the game.

I'd like to add the "ten moves to victory" or something, but that
might be hard, just because of how negamax is set up.

Wrote a benchmarks file, and created a directory to store
auto-generated benchmark results.

I had a bit of a problem with getting file handles to open in
Scheme... I forgot that I was closing the log file at the end of the
benchmarks file, so I kept getting error messages when I tried to
write to the file from the REPL. The error messages were not very
helpful... that's something that Perl is pretty good with. Scheme's
error messages are usually a bit more cryptic.
.

Start: Fri Nov 15 13:20:46 MST 2013
Stop:  Fri Nov 15 16:15:46 MST 2013

Getting some advice from Fred on how neural networks actually
work. See notes.org.

Reflections: Fred seems to think that neural networks are probably not
the best choice for what I'm doing. The biggest problem is that a
neural network is typically used for yes/no situations. I need to
extract a score from the neural network.

Fred gave me a referance to a corpus of training data that I can use
to make sure my neural networks are running correctly:
  http://archive.ics.uci.edu/ml/datasets/Iris

Really good book I'll have to check out: "Neural Smithing" by Russell
D. Reed and Robert J. Marks II.
.

Start: Mon Nov 18 21:06:58 MST 2013
Stop:

I need to make a list of features that my neural net should look
at.

Features:
  - number of white pawns and kings vs. number of black pawns and
    kings (2)
  - number of pawns within two moves of back row (2)
  - number of endangered pieces (2)
    + This will be good because it will mimic the quiescent score checker
  - number of possible moves (you and opponent) (2)
  - maximum number of consecutive empty rows (1)
  - number of opponent pieces in front of your pawns (2)
  - opponent's proximity to your pawns and kings (2)

So far, 13 inputs total.

I'll have to work to make these routines super-optimized.

TODO: find a way to use the data loaded from NN_DATA/network.scm with
my define-feed-forward-net macro. I might have to make a network, then
destructively modify the network with the stored values after I've
loaded them.

TODO: IMPORTANT!!! finish figuring out how to notate the network in
NN_DATA/network.scm
DO THIS FIRST!!

TODO: Add mechanism to automatically back up network states. Maybe
every time I load the network have it back up the file with the
timestamp.
.

Start: Tue Nov 19 11:58:34 MST 2013
Stop:  Tue Nov 19 12:05:07 MST 2013

Finished notating the network storage. Here's what the file looks
like: <<__NETWORK_SCM__
(0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0 0) ;outputs
((0 0 0 0 0 0 0 0 0 0 0 0 0)			      ;hidden layers
 (0 0 0 0 0 0 0 0 0 0 0 0 0)
 (0 0 0 0 0 0 0 0 0 0 0 0 0)
 (0 0 0 0 0 0 0 0 0 0 0 0 0)
 (0 0 0 0 0 0 0 0 0 0 0 0 0)
 (0 0 0 0 0 0 0 0 0 0 0 0 0)
 (0 0 0 0 0 0 0 0 0 0 0 0 0)
 (0 0 0 0 0 0 0 0 0 0 0 0 0)
 (0 0 0 0 0 0 0 0 0 0 0 0 0)
 (0 0 0 0 0 0 0 0 0 0 0 0 0)
 (0 0 0 0 0 0 0 0 0 0 0 0 0)
 (0 0 0 0 0 0 0 0 0 0 0 0 0)
 (0 0 0 0 0 0 0 0 0 0 0 0 0)
 (0 0 0 0 0 0 0 0 0 0 0 0 0)
 (0 0 0 0 0 0 0 0 0 0 0 0 0)
 (0 0 0 0 0 0 0 0 0 0 0 0 0)
 (0 0 0 0 0 0 0 0 0 0 0 0 0)
 (0 0 0 0 0 0 0 0 0 0 0 0 0)
 (0 0 0 0 0 0 0 0 0 0 0 0 0)
 (0 0 0 0 0 0 0 0 0 0 0 0 0)
 (0 0 0 0 0 0 0 0 0 0 0 0 0)
 (0 0 0 0 0 0 0 0 0 0 0 0 0)
 (0 0 0 0 0 0 0 0 0 0 0 0 0)
 (0 0 0 0 0 0 0 0 0 0 0 0 0)
 (0 0 0 0 0 0 0 0 0 0 0 0 0)
 (0 0 0 0 0 0 0 0 0 0 0 0 0))
__NETWORK_SCM__

Looks like lunch is just about over, so I need to go now.
.

Start: Wed Nov 20 07:45:00 MST 2013
Stop:  Wed Nov 20 08:21:50 MST 2013

Meta: TODO

Working on writing the feature detectors. I finished writing the code
to load the neural networks from file. I haven't tested this code yet,
however. Also, I need to write code to save the trained network and
back up the old network.

Oh... new problem: Exactly what values will I use to train the
network? Do I simply score the board with Negamax and then scale the
result between -1 and 1? That could work. I /think/ that will
work. I'll give it a shot.

Also, I need to get a server to do some heavy training.

I've implemented pawn and king count comparisons and advancement
potential. I'm currently working on counting the number of available
moves for a particular side (implemented) and the number of pieces in
danger of being jumped.
.

Start: Thu Nov 21 19:12:42 MST 2013
Stop:  Thu Nov 21 20:38:55 MST 2013

Meta: IMPROVEMENTS, TODO

I've finished the rough draft for the feature detectors. I'm going to
need to write some tests to make sure that this is really working.

I've got the basic feature detectors working! I don't know how
efficent they are; I'm sure I could up the performance on these
routines.

I might need to find more features about the board and implement
them. I've been thinking about the following:
  - how many enemy pieces are in front of my pieces
  - how many endangered kings and pawns (like the one I've got, except
    it separates kings and pawns)

TODO: I need to add more tests for jump chains. Here's some data to
help with testing:

     1   2   3   4   5   6   7   8
   +---+---+---+---+---+---+---+---+
 1 | . |   | . |   | . |   | . |   |
   +---+---+---+---+---+---+---+---+
 2 |   | . |   | . | B | . | b | . |
   +---+---+---+---+---+---+---+---+
 3 | . |   | . |   | . |   | . | w |
   +---+---+---+---+---+---+---+---+
 4 | w | . | b | . |   | . | b | . |
   +---+---+---+---+---+---+---+---+
 5 | . |   | . |   | . | b | . |   |
   +---+---+---+---+---+---+---+---+
 6 | b | . | b | . | w | . | W | . |
   +---+---+---+---+---+---+---+---+
 7 | . |   | . | b | . |   | . |   |
   +---+---+---+---+---+---+---+---+
 8 | b | . | b | . |   | . |   | . |
   +---+---+---+---+---+---+---+---+

#(#(0 0 0 0) #(0 0 -2 -1) #(0 0 0 1) #(1 -1 0 -1) #(0 0 -1 0) #(-1 -1 1 2) #(0 -1 0 0) #(-1 -1 0 0))


     1   2   3   4   5   6   7   8
   +---+---+---+---+---+---+---+---+
 1 | . |   | . |   | . |   | . |   |
   +---+---+---+---+---+---+---+---+
 2 | w | . |   | . |   | . | w | . |
   +---+---+---+---+---+---+---+---+
 3 | . | b | . |   | . | B | . | w |
   +---+---+---+---+---+---+---+---+
 4 | w | . |   | . | b | . |   | . |
   +---+---+---+---+---+---+---+---+
 5 | . | w | . |   | . | b | . | b |
   +---+---+---+---+---+---+---+---+
 6 | b | . | b | . |   | . |   | . |
   +---+---+---+---+---+---+---+---+
 7 | . | b | . | b | . | W | . |   |
   +---+---+---+---+---+---+---+---+
 8 | b | . | b | . |   | . |   | . |
   +---+---+---+---+---+---+---+---+

#(#(0 0 0 0) #(1 0 0 1) #(-1 0 -2 1) #(1 0 -1 0) #(1 0 -1 -1) #(-1 -1 0 0) #(-1 -1 2 0) #(-1 -1 0 0))

     1   2   3   4   5   6   7   8
   +---+---+---+---+---+---+---+---+
 1 | . |   | . |   | . |   | . |   |
   +---+---+---+---+---+---+---+---+
 2 | w | . |   | . | B | . | w | . |
   +---+---+---+---+---+---+---+---+
 3 | . |   | . |   | . | w | . | w |
   +---+---+---+---+---+---+---+---+
 4 | w | . | w | . | b | . | w | . |
   +---+---+---+---+---+---+---+---+
 5 | . | w | . | b | . |   | . | b |
   +---+---+---+---+---+---+---+---+
 6 | b | . | b | . |   | . | b | . |
   +---+---+---+---+---+---+---+---+
 7 | . | b | . | b | . | b | . |   |
   +---+---+---+---+---+---+---+---+
 8 | b | . | b | . |   | . |   | . |
   +---+---+---+---+---+---+---+---+

#(#(0 0 0 0) #(1 0 -2 1) #(0 0 1 1) #(1 1 -1 1) #(1 -1 0 -1) #(-1 -1 0 -1) #(-1 -1 -1 0) #(-1 -1 0 0))

That's all for tonight.
.

Start: Fri Nov 22 14:23:12 MST 2013
Stop:  Fri Nov 22 15:02:00 MST 2013

I need to encode the rest of the tests today, as well as impelement
some new feature detectors.

Another extremely important feature: who's turn it is to move.

Oh fie. How many output nodes should I have? I'll need something to
sort moves by. I guess I could have it return a score: 1 good for
white, -1 good for black, etc. I'm not quite sure how well training
that will go. I'll give it a shot.

As to the number of nodes... I'm not quite sure.
.
