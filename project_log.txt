Start: Wed Sep 11 08:00:02 MDT 2013
Stop:  Wed Sep 11 08:20:49 MDT 2013
Total: ~0:20

This is to help me keep track of the project, and so that I don't have
to clutter up the git commit with these longer entries.

Today I'm working on implementing the backprop algorithm. I'm using
that PDF series I found. Once I get that working I'll write some tests
using the examples in the PDF.
.

Start: Thu Sep 12 17:29:03 MDT 2013
Stop:  Thu Sep 12 20:32:24 MDT 2013
Subtractions: 1:00
Total: ~2:00

Finished writing a crude back propogation algorithm! The interface is
very messy, but it passed the tests. Now that I've got the very basics
of the network working, I'll try my network model out on some more
complicated examples, and test doing "real" learning with it. For fun,
I could teach it to play tic-tac-toe.

I'm wanting to rewrite the network simulator. The model I've got right
now is kinda messy and probably fairly inefficent. I think I'll try
writing another closure-based network system, but this time do it
better.

Eventually, I'd like to write the network with closures that compile
into some nice, fast code. I might wait to do this until after I've
got other elements of this project working, such as the chess engine
and the Negascout framework.
.

Start: Sat Sep 14 17:42:30 MDT 2013
Stop:  Sat Sep 14 20:34:51 MDT 2013
Total: ~3:00

I didn't like how ugly the closure nodes looked, so I started working
on a version that uses scheme macros. To do this, I picked a syntax
for the networks that I liked better than the old syntax, and then
basically wrote my own Domain Specific Language to implement the new
syntax.

I haven't ever done much work with Scheme's pattern matching
macros. I've used Common Lisp's macros extensivly, and thanks to Paul
Graham's book "On Lisp", I got to the point where I wrote an extremely
complex macro system that allowed me to write a very concise natural
language parser. (By concise I mean it had minimal boiler plate and
was easy to read.) The macros I'm writing for this are the largest
I've written to date (in Scheme of course), but they are far from the
most complex out there.
.

Start: Sat Sep 14 21:17:45 MDT 2013
Stop:  Sat Sep 14 23:25:43 MDT 2013
Subtractions: 0:10
Total: ~2:00

I got back propagation working with my macro tests!! For one day, I've
done a lot. I'm going to go to bed now...
.

Start: Mon Sep 16 20:55:00 MDT 2013
Stop:  Mon Sep 16 22:00:47 MDT 2013
Total: 1:05

Working on doing character recognition. Not going so well... I don't
know what's wrong. I need to debug this before I move onto my chess
engines etc.
.

Start: Tue Sep 17 19:41:31 MDT 2013
Stop:  Tue Sep 17 20:57:54 MDT 2013
Subractions: 0:00
Total: ~1:00

Debugging the character recognition net. I wrote some simpler pattern
learning networks, which worked. I'm going to get a server and train
my character recognizer heavily.
.

Start: Wed Sep 18 08:08:18 MDT 2013
Stop:  Wed Sep 18 08:21:49 MDT 2013
Total: 0:13

Worked on compiling the net into closures.
.

Start: Thu Sep 19 19:16:39 MDT 2013
Stop:  Thu Sep 19 19:21:22 MDT 2013
Start: Thu Sep 19 20:32:48 MDT 2013
Stop:  Thu Sep 19 22:19:47 MDT 2013
Total: ~1:45

Worked on debugging the new system. I've fixed quite a few minor bugs,
but I have no idea as to how many I've got left. Right now the values
for nodes (at least the output nodes) are not being set
correctly. Obviously, this is a problem as I query those nodes to get
the output of the neural net. I'm suspecting the problem is where I
have the logic for getting node values. (See the closure construction
of the nodes.)
.

Start: Fri Sep 20 20:03:01 MDT 2013
Stop:  Fri Sep 20 21:45:30 MDT 2013

Major debug session. Still not working. I'm absolutly mystified with a
particular problem: either I'm missing a very subtle bug with my usage
of `case', or the `case' macro (or even worse, something even more
low-level) has a bug in it. The bug appeared when I added a `let'
arround the place where I'm using `case'... I'm considering replacing
my usage of `case' with a `cond' expression so I can poke around in
the internals a bit more.
.

Start: Sat Sep 21 15:55:32 MDT 2013
Stop:  Sat Sep 21 17:38:02 MDT 2013

Got compiling into closures working. I need to do a speed trial, and
try out my character recognition with the new system.

That was one heck of a debug session!
.

Start: Sat Sep 21 20:51:32 MDT 2013
Stop:  Sat Sep 21 21:40:02 MDT 2013

Going to do some profiling.

...

I am absolutely mystified as to why on earth the new compiled macros
are running slower than the old ones. ?!! I need a profiler. Bad.
.

Start: Tue Sep 24 17:57:57 MDT 2013
Stop:  Tue Sep 24 19:10:03 MDT 2013

I'm going to try writing my feed-forward networks with matricies. This
should be pretty dang fast if I can code it right.
.

Start: Wed Sep 25 07:40:15 MDT 2013
Stop:  Wed Sep 25 08:20:56 MDT 2013

More work on ff-networks with matricies. There are no native matricies
for MIT Scheme, so I have to implement them myself with the native
vectors.
.

Start: Thu Sep 26 18:33:58 MDT 2013
Stop:  Thu Sep 26 20:02:00 MDT 2013

Working on implementing the backprop algorithm with matricies.
.

Start: Mon Sep 30 16:12:20 MDT 2013
Stop:  Mon Sep 30 19:17:13 MDT 2013

Ditto.

I'm still getting some errors. My calculation of the value
"forward-errors" on line 152 does not match the correct
algorithm. Compare with (δαWAα + δβWAβ) and friends on page 19 of
chapter 3 of the neural net tutorial PDFs.

I'm too tired right now to figure out the correct matrix
computations.
.

Start: Tue Oct  1 20:28:32 MDT 2013
Stop:  Tue Oct  1 21:08:36 MDT 2013
Start: Tue Oct  1 21:19:18 MDT 2013
Stop:  Tue Oct  1 21:24:52 MDT 2013

Still working on backprop algorithm.

IT'S WORKING!!! I think...

Nope. Something is still wrong.
.

Start: Wed Oct  2 07:42:59 MDT 2013
Stop:  Wed Oct  2 08:22:21 MDT 2013
Start: Wed Oct  2 09:26:32 MDT 2013
Stop:  Wed Oct  2 10:07:28 MDT 2013

It /looks/ like the mathematics are correct, but for some reason the
numbers are off after about 3 decimal places. Still not learning.

I think the decimal place bug is due to I'm working at a much higher
precision.
.

Start: Wed Oct  2 13:15:21 MDT 2013
Stop:  Wed Oct  2 13:21:34 MDT 2013
Start: Wed Oct  2 17:49:16 MDT 2013
Stop:  Wed Oct  2 18:52:55 MDT 2013

Rewriting the back prop algorithm. The code is a /tremendous/ amount
cleaner than the last one. See the "BUG:" note for my current
problem.
.

Start: Wed Oct 16 18:38:31 MDT 2013
Stop:  Wed Oct 16 18:50:31 MDT 2013
Start: Wed Oct 16 20:45:54 MDT 2013
Stop:  Wed Oct 16 23:21:17 MDT 2013

Recommencing work on progress after a lengthy break.

I've got learning with matricies working... sort of. The network is
only learning the first pattern given to it, which makes me think that
the error is caused by me not updating the values or the targets of
nodes somewhere...
I did some more checking, and I got different results from what I was
expecting: I trained the net once right at the start with pattern 1.
Then I trained it 1000 times with pattern 0. I expected that the
net would kinda learn pattern 1, and not learn pattern 0. However, the
network learned pattern 0, but returned roughly the same result for
pattern 1. I still think it's side effects.

I HAVE DONE SOMETHING VERY WRONG WITH MY DATA STRUCTURES!! Two x-rays:

Before /any/ activity:
1 ]=> (test-net01 'x-ray)
-------------- FULL DUMP FOR: 'test-net01' --------------
LAYER DUMP:
#(.1 .1 .1 .1)#(.1 .1 .1 .1)#(.1 .1 .1 .1)#(.1 .1 .1 .1)#(.1 .1 .1 .1)
#(#(.1 .1 .1 .1 .1) #(.1 .1 .1 .1 .1) #(.1 .1 .1 .1 .1) #(.1 .1 .1 .1
.1))
ERROR DUMP:
#(0 0 0 0)#(0 0 0 0)#(0 0 0 0)#(0 0 0 0)#(0 0 0 0)
#(0 0 0 0)
VALUE DUMP:
#(0 0 0 0)#(0 0 0 0)#(0 0 0 0)#(0 0 0 0)#(0 0 0 0)
#(#(0 0 0 0 0) #(0 0 0 0 0) #(0 0 0 0 0) #(0 0 0 0 0))
Reverse lists are up-to-date.
;Unspecified return value

1 ]=> (train-01-0 1)
;Value: #t

1 ]=> (test-net01 'x-ray)
-------------- FULL DUMP FOR: 'test-net01' --------------
LAYER DUMP:
#(.10668521032893902 .1 .1 .10668521032893902)#(.10668521032893902 .1
.1 .10668521032893902)#(.10668521032893902 .1 .1
.10668521032893902)#(.10668521032893902 .1 .1
.10668521032893902)#(.10668521032893902 .1 .1 .10668521032893902)
#(#(.15823362381053097 .15823362381053097 .15823362381053097
.15823362381053097 .15823362381053097)
  #(2.3340011090897797e-2 2.3340011090897797e-2 2.3340011090897797e-2
  2.3340011090897797e-2 2.3340011090897797e-2)
  #(2.3340011090897797e-2 2.3340011090897797e-2 2.3340011090897797e-2
  2.3340011090897797e-2 2.3340011090897797e-2)
  #(.15823362381053097 .15823362381053097 .15823362381053097
  .15823362381053097 .15823362381053097))
ERROR DUMP:
6.685210328939012e-36.685210328939012e-36.685210328939012e-36.685210328939012e-36.685210328939012e-3
#(.10591128248738689 -.13942387935960118 -.13942387935960118
.10591128248738689)
VALUE DUMP:
.549833997312478.549833997312478.549833997312478.549833997312478.549833997312478
#(.5682996204455919 .5682996204455919 .5682996204455919
.5682996204455919)
Reverse lists are up-to-date.
;Unspecified return value

Notice how before any activity, the values of _a_ layer are held in a
list of arrays. But after training, the values of _a_ layer are held
in a flat list! (Also note how they are all the same: this, I think is
very wrong. (Maybe not for the first training session... or maybe
yes. I don't know.)) Values should be a flat list, and errors need to
be a list of arrays.

OK, this is interesting. Every other training session, the layer
values are different from one another! Hhmmm...

1 ]=> (train-01 10)
Layer: #(.549833997312478 .549833997312478 .549833997312478 .549833997312478 .549833997312478)
Layer: #(.5682996204455919 .5682996204455919 .5682996204455919 .5682996204455919)
Layer: #(.549833997312478 .549833997312478 .549833997312478 .549833997312478 .549833997312478)
Layer: #(.6070696428491587 .5160359129077615 .5160359129077615 .6070696428491587)
Layer: #(.5531411451842421 .5531411451842421 .5531411451842421 .5531411451842421 .5531411451842421)
Layer: #(.5541421639397078 .5617705501752163 .5617705501752163 .5541421639397078)
Layer: #(.5497040086343132 .5497040086343132 .5497040086343132 .5497040086343132 .5497040086343132)
Layer: #(.5947277487620963 .5091361718820695 .5091361718820695 .5947277487620963)
Layer: #(.5564459877924409 .5564459877924409 .5564459877924409 .5564459877924409 .5564459877924409)
Layer: #(.5421537279531315 .5559192168507334 .5559192168507334 .5421537279531315)
Layer: #(.5499970779641044 .5499970779641044 .5499970779641044 .5499970779641044 .5499970779641044)
Layer: #(.5844245663272676 .5030009403207444 .5030009403207444 .5844245663272676)
Layer: #(.5597310060673503 .5597310060673503 .5597310060673503 .5597310060673503 .5597310060673503)
Layer: #(.532084739142932 .5506905464203923 .5506905464203923 .532084739142932)
Layer: #(.5506131434950248 .5506131434950248 .5506131434950248 .5506131434950248 .5506131434950248)
Layer: #(.5758942885575231 .49754429270011086 .49754429270011086 .5758942885575231)
Layer: #(.5629912680400353 .5629912680400353 .5629912680400353 .5629912680400353 .5629912680400353)
Layer: #(.5236843565380089 .5460310707424173 .5460310707424173 .5236843565380089)
Layer: #(.5514706684153556 .5514706684153556 .5514706684153556 .5514706684153556 .5514706684153556)
Layer: #(.568886012495078 .49268919231028063 .49268919231028063 .568886012495078)
;Value: #t

...

It's learning. The bug is... in my format command. How, what?!!
.

Start: Thu Oct 17 22:11:47 MDT 2013
Stop:  Thu Oct 17 22:22:55 MDT 2013

Going to quickly try to do a time trial with my new network...

Oooohhhhh baby... this is /beautiful/. I think the only more beautiful
performance comparison was the syntactic analyzer vs. the metacircular
scheme interpreter Alex and I wrote last year. Results:

   Compiler
Run time : 42470
GC Time  : 300
Real Time: 42961

    Matrix
Run time : 2810
GC Time  : 30
Real Time: 2846

An order of magnitude. Dag, yo. Win.
.

Start: Fri Oct 18 13:17:03 MDT 2013
Stop:  Fri Oct 18 13:50:53 MDT 2013

Character recognition works!! 1000 training cycles on the first for
letter bitmaps. Woot!!
.

Start: Tue Oct 22 08:00:11 MDT 2013
Stop:  Tue Oct 22 08:20:20 MDT 2013

Started work on board engine.
Notes: 0 means no piece. 1 means pawn. 2 means king.
Using bitmaps for white and black.
.

Start: Tue Oct 22 18:54:25 MDT 2013
Stop:  Tue Oct 22 19:14:11 MDT 2013
Start: Tue Oct 22 19:55:21 MDT 2013
Stop:  Tue Oct 22 20:31:30 MDT 2013

Continuing work on checkers engine.
Revised design: one bitmap, negative values represent black.
Still working on board printing.

Board prints correctly. (At least with non-kinged pieces)
.

Start: Wed Oct 23 07:44:49 MDT 2013
Stop:  Wed Oct 23 08:21:06 MDT 2013

Worked on asserting that a move is legal.
.

Start: Mon Oct 28 18:37:59 MDT 2013
Stop:  Mon Oct 28 19:03:23 MDT 2013
Start: Mon Oct 28 19:39:28 MDT 2013
Stop:  Mon Oct 28 19:48:27 MDT 2013

Going to try finishing move legality checking, etc.

This is what the board will look like:

     1   2   3   4   5   6   7   8
   +---+---+---+---+---+---+---+---+
 1 |   | w |   | w |   | w |   | w |
   +---+---+---+---+---+---+---+---+
 2 | w |   | w |   | w |   | w |   |
   +---+---+---+---+---+---+---+---+
 3 |   | w |   | w |   | w |   | w |
   +---+---+---+---+---+---+---+---+
 4 |   |   |   |   |   |   |   |   |
   +---+---+---+---+---+---+---+---+
 5 |   |   |   |   |   |   |   |   |
   +---+---+---+---+---+---+---+---+
 6 | b |   | b |   | b |   | b |   |
   +---+---+---+---+---+---+---+---+
 7 |   | b |   | b |   | b |   | b |
   +---+---+---+---+---+---+---+---+
 8 | b |   | b |   | b |   | b |   |
   +---+---+---+---+---+---+---+---+
.

Start: Tue Oct 29 20:05:03 MDT 2013
Stop:  Tue Oct 29 21:33:32 MDT 2013

I finished the first draft of the move making routines. Writing some
tests for that now...

Very basic tests in place. The last one is failing; I'm not sure
why. See the error message when engine_test.t is run.
.

Start: Wed Oct 30 07:45:43 MDT 2013
Stop:  Wed Oct 30 08:22:42 MDT 2013

Much movement seems to be working. See bug in engine_test.t: piece not
being removed from the board after being jumped.
Also todo: implement kinging, make sure that kings move properly, and
chained move legality checking (chains only legal if consecutive jumps.)
.

Start: Wed Oct 30 21:05:31 MDT 2013
Stop:  Wed Oct 30 21:12:23 MDT 2013
Start: Wed Oct 30 21:28:17 MDT 2013
Stop:  Wed Oct 30 21:45:16 MDT 2013

Pieces get kinged, fixed bug with captured piece not being removed.
.

Start: Thu Oct 31 18:58:07 MDT 2013
Stop:  Thu Oct 31 20:20:31 MDT 2013

Fixed some bugs with legal moves in my chess engine. I'm considering
starting on the search algorithm. I said I would use NegaScout, but
I've only worked with Negamax. I might want to start with that...

Also, it's kind of late, so I'm wondering if I should start something
like this. I think I'll go play the piano for a bit...
.

Start: Fri Nov  1 14:32:52 MDT 2013
Stop:  Fri Nov  1 14:42:02 MDT 2013
Start: Fri Nov  1 15:06:28 MDT 2013
Stop:  Fri Nov  1 16:37:44 MDT 2013

Going to start working on the negamax algorithm.

Working on move generation. I haven't tested any of the functions I've
written today (possible-moves, generate-possible-moves, and
collect-coordinates.) I'll need to write `collect-diagnal-squares',
which returns the 4 diagonal squares to the square given. I could (and
probably should) make this function efficient: have it do some /very/ basic
error checking: the direction is correct, and maybe if the square is
within bounds. This can't duplicate too much of the work of
assert-legal, otherwise I loose efficency.
.

Start: Mon Nov  4 20:44:27 MST 2013
Stop:  Mon Nov  4 21:56:26 MST 2013

Continuing work on move generators. It's late, so I'm not going to
spend too much time working on it.

I fixed a bug in collect-coordinates, which I'm sure improved the
efficenty of my program.

I'll need to profile this code at some point... First, I have to get
the thing working. That is the most important part: finish this
project!!
.

Start: Wed Nov  6 07:39:56 MST 2013
Stop:  Wed Nov  6 08:20:11 MST 2013

Worked on implementing jump trees. It's close, but not quite
there. The move generator is following the jump trees correctly, but
it's not returning the possibilities in the right format. I'll have to
go back and check all my CONSes, APPENDs, and LISTs.
.

Start: Thu Nov  7 20:54:03 MST 2013
Stop:  Thu Nov  7 21:23:20 MST 2013

Fixed broken jump chains. I'm only getting the maximum jump distance.
I need to find a way to generate sub-jump chains. I could call my
sub-jumps function, and then call something like remove-duplicates...
I don't know how efficient that would be though.
.
